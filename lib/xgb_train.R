
train.xgboost <- function(dat_train, label_train, par=NULL){
  
  
  library("xgboost")
  
  ### creat model list
  model <- list()
  
  ### Train with gradient boosting model
  if(is.null(par)){
    max_depth <- 2
    eta <- 1
  } else {
    max_depth = par$depth
    eta = par$eta
    #nrounds = par$nrounds
    nthread = par$nthread
  }
  
  ### the dimension of response array is 1500000 x 4 x 3, which requires 12 classifiers
  ### this part can be parallelized
  for (i in 1:12){
    ## calculate column and channel
    c1 <- (i-1) %% 4 + 1
    c2 <- (i-c1) %/% 4 + 1
    featMat <- dat_train[, , c2]
    labMat <- label_train[, c1, c2]
    fit_xgboost <- xgboost(data=featMat, label=labMat,
                           max_depth=max_depth,
                           eta=eta,
                           nthread= nthread,
                           nrounds= 10,
                           objective="reg:linear", 
                           verbose=FALSE
    )
    model[[i]] <- list(fit=fit_xgboost)
  }
  
  return(model)
}