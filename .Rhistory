rep(1:10,2,5)
rep(1:10,5)
rep(1:10,2,3)
rep(1:10,3)
rep(1:4, 2)
rep(1:4, each=2)
rep(1:4,c=(2,2,2,2))
rep(1:4,c(2,2,2,2))
rep(1:4,c(1,2,3,2))
rep(1:4,c(1,2,3,4))
rep(1:4, each=2, len=4)
rep(1:4, each=2, len=4)
rep(1:4, each=2, time=4)
rep(1:4,2,4)
rep(1:4,4)
rep(1:4,2,5)
n_fold = floor(n/k)
# 10-fold cross validation
k = 10
n_fold = floor(n/k)
n = dim(y_train)[1]
# 10-fold cross validation
k = 10
n_fold = floor(n/k)
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
n_fold
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
rep(n_fold, k-1)
(k-1)*n_fold
c(rep(n.fold, K-1), n-(K-1)*n.fold))
c(rep(n.fold, K-1), n-(K-1)*n.fold)
c(rep(n_fold, k-1), n-(k-1)*n_fold)
n-(k-1)*n_fold
9*n_fold
sample(1,10)
sample(10,5)
c(rep(n_fold, k-1), n-(k-1)*n_fold)
a = c(rep(n_fold, k-1), n-(k-1)*n_fold)
rep(1:K,a)
rep(1:k,a)
sample(rep(1:k,a))
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
s[s==10]
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
s <- sample(rep(1:k, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
s[s==10]
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
s = sample(rep(1:k, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
s = sample(rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold)))
s[s==10]
sum[s==10]
sum(s==10)
sum(s==9)
rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold))
sum(ind==4)
ind = rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold))
sum(ind==4)
ind[160000]
sample(c(1,1,1,2,2,2,3,3,3))
sample(c(3,4,3,2,2,2,4,4,4,4))
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
# sample(c(1 for 150000 times, 2 for 150000 times, ..., 10 for 150000 times))
s = sample(rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold)))
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
# sample(c(1 for 150000 times, 2 for 150000 times, ..., 10 for 150000 times))
fold_index = sample(rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold)))
x_train[fold_index!=1,,]
dim(x_train[fold_index!=1,,])
rf_train<- function(dat, label,N){
library(randomForest)
t = proc.time()
df <- as.data.frame(cbind(dat, label))
colnames(df)[ncol(df)]<-"y"
fit <- randomForest(as.factor(y)~.,data = df, importance = TRUE,ntree = N)
train_time = (proc.time() - t)[3]
cat("Elapsed time for Training Random Forest with 500 trees is ", train_time, " seconds \n")
return(fit)
}
rf_test<- function(model, dat){
pred <-predict(model,newdata=dat)
return (pred)
}
rf_train = function(x_train, y_train, n_tree){
library(randomForest)
t = proc.time()
classifier = randomForest(x = x_train,
y = y_train,
importance = TRUE,
ntree = n_tree)
train_time = (proc.time()-t)[3]
return(classifier)
}
rf_test = function(classifier, x_test){
pred = predict(classifier, newdata=x_test)
return(pred)
}
cv_error <- rep(NA, k)
PSNR <- rep(NA, k)
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 500)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
if(!require("randomForest")){
install.packages("randomForest")
}
library(randomForest)
x_train = dat_train$feature
y_train = dat_train$label
n = dim(y_train)[1]
# 10-fold cross validation
k = 10
# n_fold: the number of samples in each fold
n_fold = floor(n/k)
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
# sample(c(1 for 150000 times, 2 for 150000 times, ..., 10 for 150000 times))
fold_index = sample(rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold)))
cv_error <- rep(NA, k)
PSNR <- rep(NA, k)
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 500)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
dim(x_train)
dim(y_train)
rf_train = function(x_train, y_train, n_tree){
library(randomForest)
t = proc.time()
train_file = c(x_train, y_train)
classifier = randomForest(Class ~ ., data = train_file,
importance = TRUE,
ntree = n_tree)
# classifier = randomForest(x = x_train,
#                         y = y_train,
#                         importance = TRUE,
#                         ntree = n_tree)
train_time = (proc.time()-t)[3]
return(classifier)
}
rf_test = function(classifier, x_test){
pred = predict(classifier, newdata=x_test)
return(pred)
}
rf_train = function(x_train, y_train, n_tree){
library(randomForest)
t = proc.time()
train_file = c(x_train, y_train)
classifier = randomForest(y_train ~ ., data = train_file,
importance = TRUE,
ntree = n_tree)
# classifier = randomForest(x = x_train,
#                         y = y_train,
#                         importance = TRUE,
#                         ntree = n_tree)
train_time = (proc.time()-t)[3]
return(classifier)
}
rf_test = function(classifier, x_test){
pred = predict(classifier, newdata=x_test)
return(pred)
}
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 500)
classifier = rf_train(training_x, training_y, n_tree = 500)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
rf_train = function(x_train, y_train, n_tree){
library(randomForest)
t = proc.time()
train_file = as.data.frame(cbind(x_train, y_train))
classifier = randomForest(y_train ~ ., data = train_file,
importance = TRUE,
ntree = n_tree)
# classifier = randomForest(x = x_train,
#                         y = y_train,
#                         importance = TRUE,
#                         ntree = n_tree)
train_time = (proc.time()-t)[3]
return(classifier)
}
rf_test = function(classifier, x_test){
pred = predict(classifier, newdata=x_test)
return(pred)
}
x_train = dat_train$feature
y_train = dat_train$label
n = dim(y_train)[1]
# 10-fold cross validation
k = 10
# n_fold: the number of samples in each fold
n_fold = floor(n/k)
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
# sample(c(1 for 150000 times, 2 for 150000 times, ..., 10 for 150000 times))
fold_index = sample(rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold)))
cv_error <- rep(NA, k)
PSNR <- rep(NA, k)
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 500)
classifier = rf_train(training_x, training_y, n_tree = 500)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 500)
classifier = rf_train(training_x, training_y, n_tree = 500)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
# PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 500)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
# PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
5%%1
5%%2
load("../output/feature_train.RData")
load("../output/feature_train.RData")
load("../output/feature_train.RData")
load("../Spring2019-Proj3-spring2019-proj3-grp6/output/feature_train.RData")
load("../Spring2019-Proj3-spring2019-proj3-grp6/output/feature_train.RData")
load("../Spring2019-Proj3-spring2019-proj3-grp6/output/feature_train.RData")
getwd()
getwd()
load("../output/feature_train.RData")
load("../output/feature_train.RData")
x_train = dat_train$feature
y_train = dat_train$label
n = dim(y_train)[1]
# 10-fold cross validation
k = 10
# n_fold: the number of samples in each fold
n_fold = floor(n/k)
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
# sample(c(1 for 150000 times, 2 for 150000 times, ..., 10 for 150000 times))
fold_index = sample(rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold)))
cv_error <- rep(NA, k)
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 50)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
# PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 10)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
# PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
n = dim(y_train)[1]
# 10-fold cross validation
k = 3
# n_fold: the number of samples in each fold
n_fold = floor(n/k)
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
# sample(c(1 for 150000 times, 2 for 150000 times, ..., 10 for 150000 times))
fold_index = sample(rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold)))
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = 10)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
# PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
source("../doc/RF_model.R")
rf_train = function(x_train, y_train, param_grid){
t = proc.time()
# train_file = as.data.frame(cbind(x_train, y_train))
#
# classifier = randomForest(y_train ~ ., data = train_file,
#                           importance = TRUE,
#                           ntree = n_tree)
#
model_list = list()
if(is.null(param_grid)){
h2o_hypergrid = list(
ntrees = 200,
mtries = 1
# sample_rate = .65
)
} else {
h2o_hypergrid = param_grid
}
search_criteria = list(strategy = "RandomDiscrete")
h2o.no_progress()
h2o.init()
### the dimension of response arrat is * x 4 x 3, which requires 12 classifiers
### this part can be parallelized
for (i in 1:12){
## calculate column and channel
c1 = (i-1) %% 4 + 1
c2 = (i-c1) %/% 4 + 1
featMat = x_train[, , c2]
labMat = y_train[, c1, c2]
trainMat = cbind(featMat, labMat)
colnames(trainMat) = c(1:9)
h2o_trainMat = as.h2o(trainMat)
x = c(1:8)
y = 9
grid <- h2o.grid(
algorithm = "randomForest",
# grid_id = "rf_grid",
x = x,
y = y,
training_frame = h2o_trainMat,
hyper_params = h2o_hypergrid,
search_criteria = search_criteria)
grid_perf <- h2o.getGrid(
grid_id = "rf_grid",
sort_by = "mse",
decreasing = FALSE)
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
model_list[i] <- list(model=best_model, grid=grid_perf)
}
train_time = (proc.time()-t)[3]
cat("Training time: ", train_time)
return(model_list)
}
load("~/Documents/Spring2019/ADS/Spring2019-Proj3-spring2019-proj3-grp6/output/feature_train.RData")
x_train = dat_train$feature
y_train = dat_train$label
x_mini = x_train[1:2000,,]
y_mini = y_train[1:2000,,]
param_grid = NULL
model_list = rf_train(x_mini, y_mini, param_grid)
library(randomForest)
library(caret)
library(h2o)
model_list = rf_train(x_mini, y_mini, param_grid)
model_list
model_list[[12]]
model_list[[12]]$MSE
model_list[[1]]@parameters
x_mini = x_train[2001:4000,,]
y_mini = y_train[2001:4000,,]
model_list = rf_train(x_mini, y_mini, param_grid)
model_list = rf_train(x_train, y_train, param_grid)
x_mini = x_train[200000,,]
y_mini = y_train[200000,,]
model_list = rf_train(x_mini, y_mini, param_grid)
x_mini = x_train[1:200000,,]
y_mini = y_train[1:200000,,]
model_list = rf_train(x_mini, y_mini, param_grid)
h2o.no_progress()
load("~/Documents/Spring2019/ADS/Spring2019-Proj3-spring2019-proj3-grp6/output/feature_train.RData")
x_train = dat_train$feature
y_train = dat_train$label
x_mini = dat_train[1:1000,,]
x_mini = x_train[1:1000,,]
y_mini = y_train[1:1000,,]
rf_train = function(x_train, y_train, n_tree=NULL){
library(randomForest)
model_list = list()
t = proc.time()
if(is.null(n_tree)){
ntree = 200
} else {
ntree <- n_tree
}
train_file = as.data.frame(cbind(x_train, y_train))
### the dimension of response arrat is * x 4 x 3, which requires 12 classifiers
### this part can be parallelized
for (i in 1:12){
## calculate column and channel
c1 = (i-1) %% 4 + 1
c2 = (i-c1) %/% 4 + 1
featMat = x_train[, , c2]
labMat = y_train[, c1, c2]
classifier = randomForest(y_train ~ ., data = train_file,
importance = TRUE,
ntree = n_tree)
# fit_gbm <- gbm.fit(x=featMat, y=labMat,
#                    n.trees=200,
#                    distribution="gaussian",
#                    interaction.depth=depth,
#                    bag.fraction = 0.5,
#                    verbose=FALSE)
model_list[[i]] <- list(fit=classifier)
}
train_time = (proc.time()-t)[3]
cat("Training time: ", train_time)
return(model_list)
}
model_list = rf_train(x_mini, y_mini)
rf_train = function(x_train, y_train, n_tree=NULL){
library(randomForest)
model_list = list()
t = proc.time()
if(is.null(n_tree)){
n_tree = 200
} else {
n_tree <- n_tree
}
train_file = as.data.frame(cbind(x_train, y_train))
### the dimension of response arrat is * x 4 x 3, which requires 12 classifiers
### this part can be parallelized
for (i in 1:12){
## calculate column and channel
c1 = (i-1) %% 4 + 1
c2 = (i-c1) %/% 4 + 1
featMat = x_train[, , c2]
labMat = y_train[, c1, c2]
classifier = randomForest(y_train ~ ., data = train_file,
importance = TRUE,
ntree = n_tree)
# fit_gbm <- gbm.fit(x=featMat, y=labMat,
#                    n.trees=200,
#                    distribution="gaussian",
#                    interaction.depth=depth,
#                    bag.fraction = 0.5,
#                    verbose=FALSE)
model_list[[i]] <- list(fit=classifier)
}
train_time = (proc.time()-t)[3]
cat("Training time: ", train_time)
return(model_list)
}
model_list = rf_train(x_mini, y_mini)
model_list
source("train_rf.R")
cv_rf <- function(X_train, y_train, k, num_tree){
### Perform cross validation on Random Forest model
### Input:
###     X_train, y_train: training data
###     k: k for k-fold cross validation
###     num_tree: number of trees
n = dim(y_train)[1]
# n_fold: the number of samples in each fold
n_fold = floor(n/k)
# rep(a,b,c): repeat a for b times, but with the constraint of the length c.
# sample(c(1 for 150000 times, 2 for 150000 times, ..., 10 for 150000 times))
fold_index = sample(rep(1:k, c(rep(n_fold, k-1), n-(k-1)*n_fold)))
cv_error <- rep(NA, k)
PSNR <- rep(NA, k)
# k-fold cross validation
for(i in 1:k){
training_x = x_train[fold_index!=i,,]
training_y = y_train[fold_index!=i,,]
validation_x = x_train[fold_index==i,,]
validation_y = y_train[fold_index==i,,]
classifier = rf_train(training_x, training_y, n_tree = ntree)
pred = rf_test(classifier, validation_x)
cv_error[i] = mean((pred - validation_y)^2)
PSNR[i] = -10*log10(mean((pred - validation_y)^2))
}
MSE_output <- c(mean(cv_error),sd(cv_error))
PSNR <- c(mean(PSNR),sd(PSNR))
return(c(MSE_output,PSNR))
}
source("train_rf.R")
source("train_rf.R")
source("~/lib/train_rf.R")
getwd()
setwd("~/Documents/Spring2019/ADS/Spring2019-Proj3-spring2019-proj3-grp6")
source("~/lib/train_rf.R")
source("lib/train_rf.R")
source("train_rf.R")
source("train_rf.R")
source("lib/train_rf.R")
